{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T07:47:34.025546Z",
     "iopub.status.busy": "2023-03-07T07:47:34.024955Z",
     "iopub.status.idle": "2023-03-07T07:47:40.593119Z",
     "shell.execute_reply": "2023-03-07T07:47:40.592378Z",
     "shell.execute_reply.started": "2023-03-07T07:47:34.025467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tcn import TCN\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(34)\n",
    "\n",
    "train_data = pd.read_csv(\"dataset/train_FD001.txt\", sep= \"\\s+\", header = None)\n",
    "test_data = pd.read_csv(\"dataset/test_FD001.txt\", sep = \"\\s+\", header = None)\n",
    "true_rul = pd.read_csv(\"dataset/RUL_FD001.txt\", sep = '\\s+', header = None)\n",
    "\n",
    "def process_targets(data_length, early_rul = None):\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))\n",
    "\n",
    "def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):\n",
    "    num_batches = np.int_(np.floor((len(input_data) - window_length)/shift)) + 1\n",
    "    num_features = input_data.shape[1]\n",
    "    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length,\n",
    "                                                                                                  num_features)\n",
    "    if target_data is None:\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "        return output_data\n",
    "    else:\n",
    "        output_targets = np.repeat(np.nan, repeats = num_batches)\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "            output_targets[batch] = target_data[(shift*batch + (window_length-1))]\n",
    "        return output_data, output_targets\n",
    "\n",
    "def process_test_data(test_data_for_an_engine, window_length, shift, num_test_windows = 1):\n",
    "    max_num_test_batches = np.int_(np.floor((len(test_data_for_an_engine) - window_length)/shift)) + 1\n",
    "    if max_num_test_batches < num_test_windows:\n",
    "        required_len = (max_num_test_batches -1)* shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, max_num_test_batches\n",
    "    else:\n",
    "        required_len = (num_test_windows - 1) * shift + window_length\n",
    "        batched_test_data_for_an_engine = process_input_data_with_targets(test_data_for_an_engine[-required_len:, :],\n",
    "                                                                          target_data = None,\n",
    "                                                                          window_length = window_length, shift = shift)\n",
    "        return batched_test_data_for_an_engine, num_test_windows\n",
    "\n",
    "\n",
    "window_length = 10\n",
    "shift = 1\n",
    "early_rul = 125            \n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "\n",
    "num_test_windows = 5     \n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "\n",
    "columns_to_be_dropped = [0,1,2,3,4,5,9,10,14,20,22,23]\n",
    "\n",
    "train_data_first_column = train_data[0]\n",
    "test_data_first_column = test_data[0]\n",
    "\n",
    "# Scale data for all engines\n",
    "scaler = MinMaxScaler(feature_range = (-1,1))\n",
    "train_data = scaler.fit_transform(train_data.drop(columns = columns_to_be_dropped))\n",
    "test_data = scaler.transform(test_data.drop(columns = columns_to_be_dropped))\n",
    "\n",
    "train_data = pd.DataFrame(data = np.c_[train_data_first_column, train_data])\n",
    "test_data = pd.DataFrame(data = np.c_[test_data_first_column, test_data])\n",
    "\n",
    "num_train_machines = len(train_data[0].unique())\n",
    "num_test_machines = len(test_data[0].unique())\n",
    "\n",
    "# Process training and test data sepeartely as number of engines in training and test set may be different.\n",
    "# As we are doing scaling for full dataset, we are not bothered by different number of engines in training and test set.\n",
    "\n",
    "# Process trianing data\n",
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    temp_train_data = train_data[train_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from training data\n",
    "    if (len(temp_train_data) < window_length):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "        \n",
    "    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)\n",
    "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets, \n",
    "                                                                                window_length = window_length, shift = shift)\n",
    "    \n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)\n",
    "\n",
    "# Process test data\n",
    "for i in np.arange(1, num_test_machines + 1):\n",
    "    temp_test_data = test_data[test_data[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Verify if data of given window length can be extracted from test data\n",
    "    if (len(temp_test_data) < window_length):\n",
    "        print(\"Test engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_data_for_an_engine, num_windows = process_test_data(temp_test_data, window_length = window_length, shift = shift,\n",
    "                                                             num_test_windows = num_test_windows)\n",
    "    \n",
    "    processed_test_data.append(test_data_for_an_engine)\n",
    "    num_test_windows_list.append(num_windows)\n",
    "\n",
    "processed_test_data = np.concatenate(processed_test_data)\n",
    "true_rul = true_rul[0].values\n",
    "\n",
    "# Shuffle training data\n",
    "index = np.random.permutation(len(processed_train_targets))\n",
    "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]\n",
    "\n",
    "# print(\"Processed trianing data shape: \", processed_train_data.shape)\n",
    "# print(\"Processed training ruls shape: \", processed_train_targets.shape)\n",
    "# print(\"Processed test data shape: \", processed_test_data.shape)\n",
    "# print(\"True RUL shape: \", true_rul.shape)\n",
    "\n",
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 83)\n",
    "# print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "# print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "# print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "# print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T07:47:40.594684Z",
     "iopub.status.busy": "2023-03-07T07:47:40.594497Z",
     "iopub.status.idle": "2023-03-07T07:47:40.601384Z",
     "shell.execute_reply": "2023-03-07T07:47:40.600838Z",
     "shell.execute_reply.started": "2023-03-07T07:47:40.594665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# 创建TCN模型\n",
    "def create_tcn_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    TCN(input_shape=[window_length,processed_train_data.shape[2]],\n",
    "            nb_filters=32,\n",
    "            kernel_size=2,\n",
    "            nb_stacks=1,\n",
    "            dilations=(1, 2, 4, 8),\n",
    "            padding='causal',\n",
    "            use_skip_connections=True,\n",
    "            dropout_rate=0.05,\n",
    "            return_sequences=False,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=True,\n",
    "            use_weight_norm=False),\n",
    "            tf.keras.layers.Dense(1)\n",
    "            ])\n",
    "    model.compile(loss = \"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "# 交叉函数\n",
    "def crossover(weights1, weights2):\n",
    "    crossover_point = random.randint(0, len(weights1))\n",
    "    weights1[:crossover_point], weights2[:crossover_point] = weights2[:crossover_point], weights1[:crossover_point]\n",
    "    return weights1\n",
    "\n",
    "# 变异函数\n",
    "def mutate(weights):\n",
    "    t = random.randint(0, len(weights)-1)\n",
    "    weights[t] *= 0.9 \n",
    "    return weights\n",
    "\n",
    "#计算RMSE\n",
    "def get_RMSE(model):\n",
    "    rul_pred = model.predict(processed_test_data).reshape(-1)\n",
    "    preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "    mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights = np.repeat(1/num_windows, num_windows)) \n",
    "                             for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)]\n",
    "    RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "    return RMSE\n",
    "\n",
    "def get_RMSE_train(model):\n",
    "    rul_pred = model.predict(processed_train_data).reshape(-1)\n",
    "    preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "    mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights = np.repeat(1/num_windows, num_windows)) \n",
    "                             for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)]\n",
    "    RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "    return RMSE\n",
    "\n",
    "def compute_s_score(rul_true, rul_pred):\n",
    "    diff = rul_pred - rul_true\n",
    "    return np.sum(np.where(diff < 0, np.exp(-diff/13)-1, np.exp(diff/10)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T07:47:40.602210Z",
     "iopub.status.busy": "2023-03-07T07:47:40.602051Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义种群\n",
    "pop_size = 25\n",
    "pop = []\n",
    "\n",
    "# 定义遗传算法的参数\n",
    "parent_selection_pressure = 0.5\n",
    "mutation_rate = 0.1\n",
    "loss_tar = 13.5\n",
    "\n",
    "predict_train = []\n",
    "predict_val = []\n",
    "\n",
    "# 初始化权重\n",
    "ori_weights = []\n",
    "for i in range(pop_size):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_tcn_model()\n",
    "    ori_weights.append(model.get_weights())\n",
    "\n",
    "# 生成第一代\n",
    "print(\"第1代\")\n",
    "fitness = []\n",
    "for i in range(pop_size):\n",
    "    model.set_weights(ori_weights[i])\n",
    "    model.compile(optimizer='adam', loss='mse',metrics=[rmse])\n",
    "    print(f\"Individual_{i+1}:\")\n",
    "    model.fit(processed_train_data, processed_train_targets, epochs = 4,\n",
    "                    validation_data = (processed_val_data, processed_val_targets),\n",
    "                    batch_size = 16, verbose = 1)\n",
    "    loss = get_RMSE(model)\n",
    "    pop.append(model.get_weights())\n",
    "    fitness.append(1/loss)\n",
    "\n",
    "model.set_weights(pop[np.argmax(fitness)])\n",
    "\n",
    "generation = 1\n",
    "    \n",
    "num_generations = 10  \n",
    "for generation in range(2, num_generations + 1):\n",
    "    #选择父母\n",
    "    parents = []\n",
    "    for i in range(pop_size):\n",
    "        #轮盘赌徒法\n",
    "        idx1 = np.random.choice(np.arange(pop_size), p=fitness/np.sum(fitness))\n",
    "        idx2 = np.random.choice(np.arange(pop_size), p=fitness/np.sum(fitness))\n",
    "        if fitness[idx1] > fitness[idx2]:\n",
    "            parents.append(pop[idx1])\n",
    "        else:\n",
    "            parents.append(pop[idx2])\n",
    "    # 产生新的后代\n",
    "    offspring = []\n",
    "    for i in range(pop_size):\n",
    "        # 交叉\n",
    "        if np.random.random() < parent_selection_pressure:\n",
    "            weight1 = parents[i]\n",
    "            weight2 = parents[(i+1) % pop_size]\n",
    "            offspring.append(crossover(weight1,weight2))\n",
    "        # 变异\n",
    "        elif np.random.random() < mutation_rate:\n",
    "            offspring.append(mutate(parents[i]))\n",
    "        # 直接复制\n",
    "        else:\n",
    "            offspring.append(parents[i])\n",
    "\n",
    "    # 替换种群  \n",
    "    pop = offspring\n",
    "\n",
    "    # 计算RMSE\n",
    "    print(f\"第{generation}代\")\n",
    "    fitness = []\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        model.set_weights(pop[i])\n",
    "        model.compile(optimizer='adam', loss='mse',metrics=[rmse])\n",
    "        print(f\"Individual_{i+1}:\")\n",
    "        model.fit(processed_train_data, processed_train_targets, epochs = 4,\n",
    "                        validation_data = (processed_val_data, processed_val_targets),\n",
    "                        batch_size = 16, verbose = 1)\n",
    "        loss = get_RMSE(model)\n",
    "        pop[i] = model.get_weights()\n",
    "        fitness.append(1/loss)\n",
    "\n",
    "\n",
    "\n",
    "    # 选择最优解\n",
    "    model.set_weights(pop[np.argmax(fitness)])\n",
    "    loss = get_RMSE(model)\n",
    "    print(loss)\n",
    "\n",
    "\n",
    "print(f\"best loss:{loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rul_pred = model.predict(processed_test_data).reshape(-1)\n",
    "preds_for_each_engine = np.split(rul_pred, np.cumsum(num_test_windows_list)[:-1])\n",
    "mean_pred_for_each_engine = [np.average(ruls_for_each_engine, weights = np.repeat(1/num_windows, num_windows)) \n",
    "                             for ruls_for_each_engine, num_windows in zip(preds_for_each_engine, num_test_windows_list)]\n",
    "MAE = mean_absolute_error(true_rul, mean_pred_for_each_engine)\n",
    "MSE =  mean_squared_error(true_rul, mean_pred_for_each_engine)\n",
    "RMSE = np.sqrt(mean_squared_error(true_rul, mean_pred_for_each_engine))\n",
    "print(\"MAE: \", MAE)\n",
    "print(\"MSE: \", MSE)\n",
    "print(\"RMSE: \", RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_last_examples = np.cumsum(num_test_windows_list) - 1\n",
    "preds_for_last_example = np.concatenate(preds_for_each_engine)[indices_of_last_examples]\n",
    "\n",
    "RMSE_new = np.sqrt(mean_squared_error(true_rul, preds_for_last_example))\n",
    "print(\"RMSE (Taking only last examples): \", RMSE_new)\n",
    "\n",
    "s_score = compute_s_score(true_rul, preds_for_last_example)\n",
    "print(\"S-score: \", s_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c43c83b515c0eb114264ebcce9660655f44eb2ac3f4d7c37b0b6609a639fb8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
